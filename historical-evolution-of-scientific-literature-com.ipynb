{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83735,"databundleVersionId":9881586,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìò Historical Evolution of AI Research - A Decade-Wise Comparative Analysis\r\n\r\n---\r\n\r\nThis notebook is part of the **Gemini 1.5 Long Context competition**, demonstrating how the model's long context window enables the analysis of a large set of scientific literature spanning decades. The goal is to uncover trends, paradigm shifts, and developments within the field of Artificial Intelligence (AI) by analyzing thousands of research papers, books, and conference proceedings from the 1970s to today.\r\n\r\n---\r\n\r\n## üìù Introduction\r\n\r\nThe **Gemini 1.5 model**, with its breakthrough large context window of **2 million tokens**, enables the processing of vast amounts of data in a single context. In this project, we leverage this capability to analyze the evolution of scientific literature in AI over the past 50 years. This analysis covers how research trends, terminologies, and paradigms have shifted from one decade to the next, culminating in the current state of the field.\r\n\r\n### Why this is important:\r\n\r\n- **Rapid Evolution**: Scientific fields evolve rapidly, and understanding the historical context is crucial for predicting future trends.\r\n- **Trend Analysis**: By analyzing research trends, we can better identify emerging technologies, shifting methodologies, and influential papers that have shaped AI's progress.\r\n- **Long Context Window**: Gemini's long context window allows us to analyze the entire history of AI research in one continuous process, preserving important contextual connections between papers published across decades.","metadata":{}},{"cell_type":"markdown","source":"## ![arxiv_icon_small.png](attachment:be9caefe-e961-4797-9dbb-b47ef32192e8.png) arXiv Dataset Overview\n\nThe arXiv dataset provides a comprehensive collection of AI research papers from various categories, including machine learning, robotics, and natural language processing. It covers a wide range of publications spanning multiple decades, offering rich metadata such as titles, abstracts, publication dates, and keywords.\n\n### Why this dataset is important:\n\n- **Historical Depth**: By covering research papers from the 1970s to the present, the dataset allows for a longitudinal study of AI's evolution.\n- **Rich Metadata**: The inclusion of detailed abstracts, keywords, and publication years enables a deep dive into trends and paradigm shifts in the field.\n- **Aligned with Gemini's Capabilities**: The structure of the dataset perfectly aligns with Gemini‚Äôs ability to process large context windows, allowing us to analyze the entire body of work continuously and preserve contextual connections over decades.\n\nThis dataset is essential for uncovering emerging technologies, influential research works, and understanding the trajectory of AI as a field.\n","metadata":{},"attachments":{"be9caefe-e961-4797-9dbb-b47ef32192e8.png":{"image/png":"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"}}},{"cell_type":"code","source":"try:\n    import arxiv\n    print(\"arxiv is already installed.\")\nexcept ImportError:\n    # If arxiv is not installed, install it\n    !pip install arxiv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T08:15:40.231601Z","iopub.execute_input":"2024-10-22T08:15:40.232465Z","iopub.status.idle":"2024-10-22T08:15:40.239407Z","shell.execute_reply.started":"2024-10-22T08:15:40.232411Z","shell.execute_reply":"2024-10-22T08:15:40.238285Z"}},"outputs":[{"name":"stdout","text":"arxiv is already installed.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Import all necessary libraries here.\nimport arxiv\nimport pandas as pd\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T08:15:40.241562Z","iopub.execute_input":"2024-10-22T08:15:40.241953Z","iopub.status.idle":"2024-10-22T08:15:40.249906Z","shell.execute_reply.started":"2024-10-22T08:15:40.241912Z","shell.execute_reply":"2024-10-22T08:15:40.248763Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## üìä Dataset Overview\nThe dataset used in this analysis contains a comprehensive collection of AI research papers spanning over 50 years. Sourced from arXiv.org and NeurIPS conference proceedings, it includes metadata such as titles, authors, abstracts, publication dates, and key terms. This structured information allows us to analyze the evolution of AI research and track paradigm shifts within the field.\n\n### Why this dataset is valuable:\n- **Comprehensive Coverage**: The dataset spans multiple decades, capturing the progression of AI research from its early stages to the present.\n- **Insightful Metadata**: With detailed abstracts, keywords, and publication dates, the dataset provides rich context for understanding research trends and identifying influential papers.\n- **Enabling Long-Context Analysis**: The structured data aligns perfectly with Gemini's capability to handle large context windows, enabling a holistic view of AI's development and preserving connections across decades of research.\n\nThis dataset is essential for identifying patterns, understanding the evolution of terminology, and uncovering the emerging technologies that shape the future of AI.","metadata":{},"attachments":{"7ee2cbe6-5c4d-4def-bafa-341ffb8e6423.png":{"image/png":"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"}}},{"cell_type":"code","source":"# Query for AI-related papers from the arXiv API\nsearch_query = 'cat:cs.AI OR cat:stat.ML OR cat:cs.LG'\n\n# Initialize an empty list to store papers\npapers = []\ntotal_results = 1000  # Total number of results you want to retrieve\nbatch_size = 100  # The maximum batch size supported by arXiv API\ncurrent_count = 0\n\n# Fetch results in batches\nfor start in range(0, total_results, batch_size):\n    search = arxiv.Search(\n        query=search_query,\n        max_results=batch_size,\n        sort_by=arxiv.SortCriterion.SubmittedDate\n    )\n\n    try:\n        # Fetch results from the search object\n        for result in search.results():\n            papers.append({\n                'title': result.title,\n                'authors': [author.name for author in result.authors],\n                'abstract': result.summary,\n                'published': result.published,\n                'categories': result.categories,\n                'pdf_url': result.pdf_url\n            })\n            current_count += 1\n            if current_count >= total_results:\n                break\n        time.sleep(3)  \n    except arxiv.UnexpectedEmptyPageError as e:\n        print(f\"Empty page encountered at start={start}. Skipping this batch.\")\n        continue \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        break\n\n# Display the total number of papers retrieved and the first few entries\nprint(f\"Total papers retrieved: {len(papers)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T08:15:40.251645Z","iopub.execute_input":"2024-10-22T08:15:40.252089Z","iopub.status.idle":"2024-10-22T08:16:46.130710Z","shell.execute_reply.started":"2024-10-22T08:15:40.252049Z","shell.execute_reply":"2024-10-22T08:16:46.129558Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3346801040.py:20: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n  for result in search.results():\n","output_type":"stream"},{"name":"stdout","text":"Total papers retrieved: 1000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# After fetching the papers, save the metadata to a CSV file for easier use in Kaggle working directory (/kaggle/working)\ndf = pd.DataFrame(papers)\ndf.to_csv('/kaggle/working/arxiv_ai_papers.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T08:16:46.133263Z","iopub.execute_input":"2024-10-22T08:16:46.133623Z","iopub.status.idle":"2024-10-22T08:16:46.197107Z","shell.execute_reply.started":"2024-10-22T08:16:46.133584Z","shell.execute_reply":"2024-10-22T08:16:46.196183Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Organize by decades to fit our analysis.\ndf = pd.read_csv('/kaggle/working/arxiv_ai_papers.csv')\n\n# Convert the published date to a datetime format\ndf['published'] = pd.to_datetime(df['published'])\n\n# Extract the year and create a decade column\ndf['year'] = df['published'].dt.year\ndf['decade'] = (df['year'] // 10) * 10\n\n# filter papers from the 1980s\ndf_1980s = df[df['decade'] == 1980]\n\nprint(f\"Printing the database from after 1980\")\nprint(df_1980s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T08:16:46.198288Z","iopub.execute_input":"2024-10-22T08:16:46.198615Z","iopub.status.idle":"2024-10-22T08:16:46.269254Z","shell.execute_reply.started":"2024-10-22T08:16:46.198580Z","shell.execute_reply":"2024-10-22T08:16:46.267813Z"}},"outputs":[{"name":"stdout","text":"Printing the database from after 1980\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m df_1980s \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecade\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1980\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrinting the database from after 1980\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_1980\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'df_1980' is not defined"],"ename":"NameError","evalue":"name 'df_1980' is not defined","output_type":"error"}],"execution_count":14}]}